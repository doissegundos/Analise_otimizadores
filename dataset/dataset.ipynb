{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Este codigo vai ler as imagens do dataset, separar elas em treino, teste e validação e salvar o arquivo CSV das imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split \n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#associa todas as imagens do path a variavel 'image_dataset_train', já convertendo para o tamanho q será utilizado\n",
    "def open_img():\n",
    "    data_train = []\n",
    "    #criando o dataset\n",
    "    for i in glob.glob(r'BreaKHis_v1/histology_slides/breast/**/*.png', recursive=True):\n",
    "      resized = cv2.resize(cv2.cvtColor(cv2.imread(i),cv2.COLOR_BGR2RGB), (128,128), interpolation = cv2.INTER_AREA)\n",
    "      data_train.append(resized)  \n",
    "    return np.array(data_train)\n",
    "image_dataset_train = open_img()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7909, 128, 128, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ler o nome das imagens e associa a um vetor\n",
    "nomes_imagens = list(pathlib.Path(r\"C:\\Users\\STEFA\\Desktop\\github\\Analise_otimizadores\\dataset\\BreaKHis_v1\").glob(r\"**/*.png\"))\n",
    "\n",
    "def converte(nomes_imagens):\n",
    "    label_dataset = []\n",
    "    labelaux1, labelaux2 = 0,0\n",
    "    for i in range(len(nomes_imagens)):\n",
    "        labelaux1 = str(nomes_imagens[i])\n",
    "        labelaux1 = labelaux1.split('\\\\S')[4]\n",
    "        #Verifica se a imagem é do tipo benigno e maligno e associa o tipo a variavel label\n",
    "        if(labelaux1[3]=='B'):\n",
    "            label = 'ben'\n",
    "        else:\n",
    "            label = 'mal'\n",
    "        \n",
    "        #Verifica o tamanho da imagem, existe os tamanhos: 40x,100x,200x,400x\n",
    "        labelaux2 = labelaux1.split('-')[3]\n",
    "        if(labelaux2 == '40'):\n",
    "            label = label + '-' + \"40\"\n",
    "        elif(labelaux2 == '100'):\n",
    "            label = label + '-' + \"100\"\n",
    "        elif(labelaux2 == '200'):\n",
    "            label = label + '-' + \"200\"\n",
    "        elif(labelaux2 == '400'):\n",
    "            label = label + '-' + \"400\"        \n",
    "        label_dataset.append(label)\n",
    "    \n",
    "    return label_dataset\n",
    "            \n",
    "label_dataset = converte(nomes_imagens)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separa os dataset das imagens, gerando 8 datasets, sendo para os 4 tamanhos e os dois tipos de imagens\n",
    "def split(image_dataset_train,label_dataset):\n",
    "    ben40,ben100,ben200,ben400 = [],[],[],[]\n",
    "    mal40,mal100,mal200,mal400 = [],[],[],[]\n",
    "    benlabel40,benlabel100,benlabel200,benlabel400 = [],[],[],[]\n",
    "    mallabel40,mallabel100,mallabel200,mallabel400 = [],[],[],[]\n",
    "    for i in range(len(image_dataset_train)):\n",
    "        if(label_dataset[i]=='ben-40'):\n",
    "            ben40.append(image_dataset_train[i])\n",
    "            benlabel40.append(0)\n",
    "            \n",
    "        elif(label_dataset[i]=='ben-100'):\n",
    "            ben100.append(image_dataset_train[i])\n",
    "            benlabel100.append(0)\n",
    "            \n",
    "        elif(label_dataset[i]=='ben-200'):\n",
    "            ben200.append(image_dataset_train[i])\n",
    "            benlabel200.append(0)\n",
    "            \n",
    "        elif(label_dataset[i]=='ben-400'):\n",
    "            ben400.append(image_dataset_train[i])\n",
    "            benlabel400.append(0)\n",
    "            \n",
    "        elif(label_dataset[i]=='mal-40'):\n",
    "            mal40.append(image_dataset_train[i])\n",
    "            mallabel40.append(1)\n",
    "            \n",
    "        elif(label_dataset[i]=='mal-100'):\n",
    "            mal100.append(image_dataset_train[i])\n",
    "            mallabel100.append(1)\n",
    "            \n",
    "        elif(label_dataset[i]=='mal-200'):\n",
    "            mal200.append(image_dataset_train[i])\n",
    "            mallabel200.append(1)\n",
    "            \n",
    "        elif(label_dataset[i]=='mal-400'):\n",
    "            mal400.append(image_dataset_train[i])\n",
    "            mallabel400.append(1)\n",
    "    return ben40,ben100,ben200,ben400,mal40,mal100,mal200,mal400,benlabel40,benlabel100,benlabel200,benlabel400,mallabel40,mallabel100,mallabel200,mallabel400\n",
    "ben40,ben100,ben200,ben400,mal40,mal100,mal200,mal400,benlabel40,benlabel100,benlabel200,benlabel400,mallabel40,mallabel100,mallabel200,mallabel400 = split(image_dataset_train,label_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasetcompleto(ben,mal,labelben,labelmal):\n",
    "    ben.extend(mal)\n",
    "    labelben.extend(labelmal)\n",
    "    return ben, labelben\n",
    "dataset_image, dataset_label = datasetcompleto(ben200,mal200,benlabel200,mallabel200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, label_train, label_test = train_test_split(dataset_image,dataset_label, test_size=0.2,random_state=1)\n",
    "data_test, data_valid, label_test, label_valid = train_test_split(data_test,label_test, test_size=0.5,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(r\"C:\\Users\\STEFA\\Desktop\\github\\Analise_otimizadores\\dataset\\npy\\200x\\datatrain200x.npy\", data_train)\n",
    "np.save(r\"C:\\Users\\STEFA\\Desktop\\github\\Analise_otimizadores\\dataset\\npy\\200x\\datatest200x.npy\", data_test)\n",
    "np.save(r\"C:\\Users\\STEFA\\Desktop\\github\\Analise_otimizadores\\dataset\\npy\\200x\\datavalid200x.npy\", data_valid)\n",
    "\n",
    "np.save(r\"C:\\Users\\STEFA\\Desktop\\github\\Analise_otimizadores\\dataset\\npy\\200x\\labeltrain200x.npy\", label_train)\n",
    "np.save(r\"C:\\Users\\STEFA\\Desktop\\github\\Analise_otimizadores\\dataset\\npy\\200x\\labeltest200x.npy\", label_test)\n",
    "np.save(r\"C:\\Users\\STEFA\\Desktop\\github\\Analise_otimizadores\\dataset\\npy\\200x\\labelvalid200x.npy\", label_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
